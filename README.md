# Plug-and-play Module
Plug and play transformer

### CVï¼š
**Survey:**
|  Name                      | Paper           | Time           |
|----------------------------| ----------------| ---------------|
| Transformers in Vision: A Survey (v1,v2) | Paper:https://arxiv.org/abs/2101.01169  |    <div style="width: 200pt">   2021-01-05    </div>     |
| Attention mechanisms and deep learning for machine vision:A survey of the state of the art   | Paper:https://arxiv.org/abs/2106.07550 |         2021-06-05       |
<br />
<br />
<br />


| Name                                        | Paper                                      | Tutorial                                                     |
| ------------------------------------------- | ------------------------------------------ | ------------------------------------------------------------ |
| 1. Squeeze-and-Excitation                   | Paper:https://arxiv.org/pdf/1709.01507.pdf | https://github.com/leader402/Plug-and-play/blob/main/cv/tutorial/SE.py |
| 2. Polarized Self-Attention                 | Paper:https://arxiv.org/pdf/2107.00782.pdf | https://github.com/leader402/Plug-and-play/blob/main/cv/tutorial/PSA.py |
| 3. Dual Attention Network                   | Paper:https://arxiv.org/pdf/1809.02983.pdf | https://github.com/leader402/Plug-and-play/blob/main/cv/tutorial/DaNet.py |
| 4. Self-attention                           |                                            |                                                              |
| 5. Masked self-attention                    |                                            |                                                              |
| 6. Multi-head attention                     |                                            |                                                              |
| 7. Attention based deep learning architectures |                                            |                                                              |
| 8. Single-channel model                     |                                            |                                                              |
| 9. Multi-channel model                      |                                            |                                                              |
| 10.Skip-layer model                         |                                            |                                                              |
| 11.Bottom-up/top-down model                 |                                            |                                                              |
| 12.CBAM: Convolutional Block Attention Module|                                            |                                                              |

